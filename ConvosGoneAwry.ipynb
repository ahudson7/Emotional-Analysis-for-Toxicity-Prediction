{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvosGoneAwry.ipynb",
      "provenance": [],
      "mount_file_id": "114N2PtJ-C2UtuX4JM1N-JakEZTOgalBp",
      "authorship_tag": "ABX9TyPpV4YPv6Cz2ijBCZqRTDsh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahudson7/Emotional-Analysis-for-Toxicity-Prediction/blob/main/ConvosGoneAwry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ctQc0CLbnN"
      },
      "source": [
        "The code in this notebook is modified from the Cornell Conversational Analysis Toolkit (aka: Convokit), which can be found at https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb.  \n",
        "\n",
        "The paper associated with this work is:\n",
        "\n",
        "Justine Zhang, Jonathan Chang, Cristian Danescu-Niculescu-Mizil, Lucas Dixon, Yiqing Hua, Dario Taraborelli, and Nithum Thain. 2018. Conversations gone awry: Detecting early signs of conversational failure. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1350–1361, Melbourne, Australia. Association for Computational Linguistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnfWvKvtJNS-"
      },
      "source": [
        "#Pre-processing Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCu7vl-0KbN8"
      },
      "source": [
        "## Step 0: Setting up workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrHvRcBeG-Ot"
      },
      "source": [
        "pip install NRCLex # not necessary since importing pre-done CSV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w_whGB2gYys"
      },
      "source": [
        "pip install convokit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HViZ40yUcMdC"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut\n",
        "from sklearn.feature_selection import f_classif, SelectPercentile\n",
        "\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from convokit import download\n",
        "from convokit.prompt_types import PromptTypeWrapper\n",
        "from convokit import PolitenessStrategies\n",
        "from convokit import Corpus\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snj_mI_FKT08"
      },
      "source": [
        "## Step 1: Loading the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2t6ofEdhr7N"
      },
      "source": [
        "# OPTION 1: DOWNLOAD CORPUS \n",
        "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
        "# DATA_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing'\n",
        "# AWRY_ROOT_DIR = download('conversations-gone-awry-corpus', data_dir=DATA_DIR)\n",
        "# AWRY_ROOT_DIR = download('conversations-gone-awry-cmv-corpus', data_dir=DATA_DIR)\n",
        "\n",
        "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
        "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE TENNIS-CORPUS IS LOCATED\n",
        "# AWRY_ROOT_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/conversations-gone-awry-corpus'\n",
        "AWRY_ROOT_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/conversations-gone-awry-cmv-corpus'\n",
        "\n",
        "awry_corpus = Corpus(AWRY_ROOT_DIR)\n",
        "awry_corpus.load_info('utterance',['parsed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLO5XBXqhkJh"
      },
      "source": [
        "awry_corpus = awry_corpus.filter_conversations_by(lambda convo: convo.meta['annotation_year'] == '2018')\n",
        "awry_corpus.print_summary_stats()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKWTGWhfLRiB"
      },
      "source": [
        "# Feature development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqRgwImBKWcq"
      },
      "source": [
        "## Step 2: Extract prompt types features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgKiI0j6ho6A"
      },
      "source": [
        "# OPTION 1: DOWNLOAD CORPUS \n",
        "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
        "# DATA_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing'\n",
        "# FULL_ROOT_DIR = download('tennis-corpus', data_dir=DATA_DIR)\n",
        "\n",
        "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
        "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE TENNIS-CORPUS IS LOCATED\n",
        "FULL_ROOT_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/tennis-corpus'\n",
        "\n",
        "full_corpus = Corpus(FULL_ROOT_DIR)\n",
        "full_corpus.load_info('utterance',['parsed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq7erLFyM6km"
      },
      "source": [
        "# We encountered an issue with this package that occurs due to a known bug. The best workaround we found was to import and install it right it is needed in the next step.\n",
        "# Sometimes it still runs into errors, and reloading the workbook and then retrying will often fix them.\n",
        "\n",
        "import spacy \n",
        "\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NnfkwXDyTaL"
      },
      "source": [
        "pt_model = PromptTypeWrapper(n_types=6, use_prompt_motifs=False, root_only=False,\n",
        "                            questions_only=False, enforce_caps=False, min_support=20, min_df=100,\n",
        "                            svd__n_components=50, max_dist=1., random_state=1000)\n",
        "pt_model.fit(full_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyslEUlOz7Kg"
      },
      "source": [
        "pt_model.summarize(full_corpus, k=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKsRyDyM0IqD"
      },
      "source": [
        "TYPE_NAMES = ['Prompt: Casual', 'Prompt: Moderation', 'Prompt: Coordination', 'Prompt: Contention',\n",
        "             'Prompt: Editing', 'Prompt: Procedures']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edmQceN80KSH"
      },
      "source": [
        "awry_corpus = pt_model.transform(awry_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "537FQFTx0OCm"
      },
      "source": [
        "prompt_dist_df = awry_corpus.get_vectors(name='prompt_types__prompt_dists.6', \n",
        "                                         as_dataframe=True)\n",
        "type_ids = np.argmin(prompt_dist_df.values, axis=1)\n",
        "mask  = np.min(prompt_dist_df.values, axis=1) >= 1.\n",
        "type_ids[mask] = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNKLc04M0RKe"
      },
      "source": [
        "prompt_dist_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633hXD9J0UYn"
      },
      "source": [
        "prompt_type_assignments = np.zeros((len(prompt_dist_df), prompt_dist_df.shape[1]+1))\n",
        "prompt_type_assignments[np.arange(len(type_ids)),type_ids] = 1\n",
        "prompt_type_assignment_df = pd.DataFrame(columns=np.arange(prompt_dist_df.shape[1]+1), index=prompt_dist_df.index, \n",
        "                                        data=prompt_type_assignments)\n",
        "prompt_type_assignment_df = prompt_type_assignment_df[prompt_type_assignment_df.columns[:-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inwBWkgs0W2a"
      },
      "source": [
        "prompt_type_assignment_df.columns = TYPE_NAMES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy4RUnBT0YS6"
      },
      "source": [
        "prompt_type_assignment_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdDwTxsQKFqd"
      },
      "source": [
        "## Step 3: Extract politeness strategies features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaWyXT3O0b3L"
      },
      "source": [
        "\n",
        "\n",
        "ps = PolitenessStrategies(verbose=1000)\n",
        "awry_corpus = ps.transform(awry_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5WXoaw0hEG"
      },
      "source": [
        "utterance_ids = awry_corpus.get_utterance_ids()\n",
        "rows = []\n",
        "for uid in utterance_ids:\n",
        "    rows.append(awry_corpus.get_utterance(uid).meta[\"politeness_strategies\"])\n",
        "politeness_strategies = pd.DataFrame(rows, index=utterance_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc2tMyQJ0kO4"
      },
      "source": [
        "politeness_strategies.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pMrLuo5KCLt"
      },
      "source": [
        "## Step 3.5: Extracting emotional information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dE5UwcqGkqr"
      },
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/CGA_emotion_df.csv'\n",
        "emotion_df = pd.read_csv(file_path, index_col = 0)\n",
        "\n",
        "'''\n",
        "from nrclex import NRCLex\n",
        "#from convokit import Corpus, download\n",
        "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
        "\n",
        "count = 0\n",
        "\n",
        "emotion_df = pd.DataFrame(columns=['fear', 'anger', 'anticipation', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy'])\n",
        "\n",
        "emotion_list = ['fear', 'anger', 'anticipation', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy']\n",
        "\n",
        "for utt in corpus.iter_utterances():\n",
        "    emotion = NRCLex(utt.text)\n",
        "    temp = []\n",
        "    for k in emotion.affect_frequencies.keys():\n",
        "        if k in emotion_list:\n",
        "            emotion_df.loc[utt.id, k] = emotion.affect_frequencies[k]\n",
        "        elif k == 'anticip':\n",
        "            emotion_df.loc[utt.id, 'anticipation'] = emotion.affect_frequencies[k]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoBFRbg4m2TK"
      },
      "source": [
        "emotion_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrZsMd8H4Iy"
      },
      "source": [
        "## Step 4: Create pair data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G0si-jR0m1N"
      },
      "source": [
        "# first, we need to directly map comment IDs to their conversations. We'll build a DataFrame to do this\n",
        "comment_ids = []\n",
        "convo_ids = []\n",
        "timestamps = []\n",
        "page_ids = []\n",
        "for conversation in awry_corpus.iter_conversations():\n",
        "    for comment in conversation.iter_utterances():\n",
        "        # section headers are included in the dataset for completeness, but for prediction we need to ignore\n",
        "        # them as they are not utterances\n",
        "        if not comment.meta[\"is_section_header\"]:\n",
        "            comment_ids.append(comment.id)\n",
        "            convo_ids.append(comment.root)\n",
        "            timestamps.append(comment.timestamp)\n",
        "            page_ids.append(conversation.meta[\"page_id\"])\n",
        "comment_df = pd.DataFrame({\"conversation_id\": convo_ids, \"timestamp\": timestamps, \"page_id\": page_ids}, index=comment_ids)\n",
        "\n",
        "# we'll do our construction using awry conversation ID's as the reference key\n",
        "awry_convo_ids = set()\n",
        "# these dicts will then all be keyed by awry ID\n",
        "good_convo_map = {}\n",
        "page_id_map = {}\n",
        "for conversation in awry_corpus.iter_conversations():\n",
        "    if conversation.meta[\"conversation_has_personal_attack\"] and conversation.id not in awry_convo_ids:\n",
        "        awry_convo_ids.add(conversation.id)\n",
        "        good_convo_map[conversation.id] = conversation.meta[\"pair_id\"]\n",
        "        page_id_map[conversation.id] = conversation.meta[\"page_id\"]\n",
        "awry_convo_ids = list(awry_convo_ids)\n",
        "pairs_df = pd.DataFrame({\"bad_conversation_id\": awry_convo_ids,\n",
        "                         \"conversation_id\": [good_convo_map[cid] for cid in awry_convo_ids],\n",
        "                         \"page_id\": [page_id_map[cid] for cid in awry_convo_ids]})\n",
        "# finally, we will augment the pairs dataframe with the IDs of the first and second comment for both\n",
        "# the bad and good conversation. This will come in handy for constructing the feature matrix.\n",
        "first_ids = []\n",
        "second_ids = []\n",
        "first_ids_bad = []\n",
        "second_ids_bad = []\n",
        "for row in pairs_df.itertuples():\n",
        "    # \"first two\" is defined in terms of time of posting\n",
        "    comments_sorted = comment_df[comment_df.conversation_id==row.conversation_id].sort_values(by=\"timestamp\")\n",
        "    first_ids.append(comments_sorted.iloc[0].name)\n",
        "    second_ids.append(comments_sorted.iloc[1].name)\n",
        "    comments_sorted_bad = comment_df[comment_df.conversation_id==row.bad_conversation_id].sort_values(by=\"timestamp\")\n",
        "    first_ids_bad.append(comments_sorted_bad.iloc[0].name)\n",
        "    second_ids_bad.append(comments_sorted_bad.iloc[1].name)\n",
        "pairs_df = pairs_df.assign(first_id=first_ids, second_id=second_ids, \n",
        "                           bad_first_id=first_ids_bad, bad_second_id=second_ids_bad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQlezI96J8Zl"
      },
      "source": [
        "## Step 5: Comparing features exhibited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7clBI4p10sjD"
      },
      "source": [
        "def clean_feature_name(feat):\n",
        "    new_feat = feat.replace('feature_politeness','').replace('==','').replace('_', ' ')\n",
        "    split = new_feat.split()\n",
        "    first, rest = split[0], ' '.join(split[1:]).lower()\n",
        "    if first[0].isalpha():\n",
        "        first = first.title()\n",
        "    if 'Hashedge' in first:\n",
        "        return 'Hedge (lexicon)'\n",
        "    if 'Hedges' in first:\n",
        "        return 'Hedge (dep. tree)'\n",
        "    if 'greeting' in feat:\n",
        "        return 'Greetings'\n",
        "    cleaner_str = first + ' ' + rest\n",
        "#     cleaner_str = cleaner_str.replace('2nd', '2$\\mathregular{^{nd}}$').replace('1st', '1$\\mathregular{^{st}}$')\n",
        "    return cleaner_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeX3unSb0yGz"
      },
      "source": [
        "politeness_strategies_display = politeness_strategies[[col for col in politeness_strategies.columns \n",
        "                  if col not in ['feature_politeness_==HASNEGATIVE==', 'feature_politeness_==HASPOSITIVE==']]].copy()\n",
        "politeness_strategies_display.columns = [clean_feature_name(col) for col in politeness_strategies_display.columns]\n",
        "\n",
        "all_features = politeness_strategies_display.join(prompt_type_assignment_df)\n",
        "\n",
        "tox_first_comment_features =pairs_df[['bad_first_id']].join(all_features, how='left', on='bad_first_id')[all_features.columns]\n",
        "ntox_first_comment_features =pairs_df[['first_id']].join(all_features, how='left', on='first_id')[all_features.columns]\n",
        "\n",
        "tox_second_comment_features =pairs_df[['bad_second_id']].join(all_features, how='left', on='bad_second_id')[all_features.columns]\n",
        "ntox_second_comment_features =pairs_df[['second_id']].join(all_features, how='left', on='second_id')[all_features.columns]\n",
        "\n",
        "\n",
        "def get_p_stars(x):\n",
        "    if x < .001: return '***'\n",
        "    elif x < .01: return '**'\n",
        "    elif x < .05: return '*'\n",
        "    else: return ''\n",
        "def compare_tox(df_ntox, df_tox,  min_n=0):\n",
        "    cols = df_ntox.columns\n",
        "    num_feats_in_tox = df_tox[cols].sum().astype(int).rename('num_feat_tox')\n",
        "    num_nfeats_in_tox = (1 - df_tox[cols]).sum().astype(int).rename('num_nfeat_tox')\n",
        "    num_feats_in_ntox = df_ntox[cols].sum().astype(int).rename('num_feat_ntox')\n",
        "    num_nfeats_in_ntox = (1 - df_ntox[cols]).sum().astype(int).rename('num_nfeat_ntox')\n",
        "    prop_tox = df_tox[cols].mean().rename('prop_tox')\n",
        "    ref_prop_ntox = df_ntox[cols].mean().rename('prop_ntox')\n",
        "    n_tox = len(df_tox)\n",
        "    df = pd.concat([\n",
        "        num_feats_in_tox, \n",
        "        num_nfeats_in_tox,\n",
        "        num_feats_in_ntox,\n",
        "        num_nfeats_in_ntox,\n",
        "        prop_tox,\n",
        "        ref_prop_ntox,\n",
        "    ], axis=1)\n",
        "    df['num_total'] = df.num_feat_tox + df.num_feat_ntox\n",
        "    df['log_odds'] = np.log(df.num_feat_tox) - np.log(df.num_nfeat_tox) \\\n",
        "        + np.log(df.num_nfeat_ntox) - np.log(df.num_feat_ntox)\n",
        "    df['abs_log_odds'] = np.abs(df.log_odds)\n",
        "    df['binom_p'] = df.apply(lambda x: stats.binom_test(x.num_feat_tox, n_tox, x.prop_ntox), axis=1)\n",
        "    df = df[df.num_total >= min_n]\n",
        "    df['p'] = df['binom_p'].apply(lambda x: '%.3f' % x)\n",
        "    df['pstars'] = df['binom_p'].apply(get_p_stars)\n",
        "    return df.sort_values('log_odds', ascending=False)\n",
        "\n",
        "first_comparisons = compare_tox(ntox_first_comment_features, tox_first_comment_features)\n",
        "second_comparisons = compare_tox(ntox_second_comment_features, tox_second_comment_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc1c40NY0-wj"
      },
      "source": [
        "# we are now ready to plot these comparisons. the following (rather intimidating) helper function \n",
        "# produces a nicely-formatted plot:\n",
        "def draw_figure(ax, first_cmp, second_cmp, title='', prompt_types=6, min_log_odds=.2, min_freq=50,xlim=.85):\n",
        "\n",
        "    # selecting and sorting the features to plot, given minimum effect sizes and statistical significance\n",
        "    frequent_feats = first_cmp[first_cmp.num_total >= min_freq].index.union(second_cmp[second_cmp.num_total >= min_freq].index)\n",
        "    lrg_effect_feats = first_cmp[(first_cmp.abs_log_odds >= .2)\n",
        "                                & (first_cmp.binom_p < .05)].index.union(second_cmp[(second_cmp.abs_log_odds >= .2)\n",
        "                                                                                  & (second_cmp.binom_p < .05)].index)\n",
        "    feats_to_include = frequent_feats.intersection(lrg_effect_feats)\n",
        "    feat_order = sorted(feats_to_include, key=lambda x: first_cmp.loc[x].log_odds, reverse=True)\n",
        "\n",
        "    # parameters determining the look of the figure\n",
        "    colors = ['darkorchid', 'seagreen']\n",
        "    shapes = ['d', 's']    \n",
        "    eps = .02\n",
        "    star_eps = .035\n",
        "    xlim = xlim\n",
        "    min_log = .2\n",
        "    gap_prop = 2\n",
        "    label_size = 14\n",
        "    title_size=18\n",
        "    radius = 144\n",
        "    features = feat_order\n",
        "    ax.invert_yaxis()\n",
        "    ax.plot([0,0], [0, len(features)/gap_prop], color='black')\n",
        "    \n",
        "    # for each figure we plot the point according to effect size in the first and second comment, \n",
        "    # and add axis labels denoting statistical significance\n",
        "    yticks = []\n",
        "    yticklabels = []\n",
        "    for f_idx, feat in enumerate(features):\n",
        "        curr_y = (f_idx + .5)/gap_prop\n",
        "        yticks.append(curr_y)\n",
        "        try:\n",
        "            \n",
        "            first_p = first_cmp.loc[feat].binom_p\n",
        "            second_p = second_cmp.loc[feat].binom_p            \n",
        "            if first_cmp.loc[feat].abs_log_odds < min_log:\n",
        "                first_face = \"white\"\n",
        "            elif first_p >= 0.05:\n",
        "                first_face = 'white'\n",
        "            else:\n",
        "                first_face = colors[0]\n",
        "            if second_cmp.loc[feat].abs_log_odds < min_log:\n",
        "                second_face = \"white\"\n",
        "            elif second_p >= 0.05:\n",
        "                second_face = 'white'\n",
        "            else:\n",
        "                second_face = colors[1]\n",
        "            ax.plot([-1 * xlim, xlim], [curr_y, curr_y], '--', color='grey', zorder=0, linewidth=.5)\n",
        "            \n",
        "            ax.scatter([first_cmp.loc[feat].log_odds], [curr_y + eps], s=radius, edgecolor=colors[0], marker=shapes[0],\n",
        "                        zorder=20, facecolors=first_face)\n",
        "            ax.scatter([second_cmp.loc[feat].log_odds], [curr_y + eps], s=radius, edgecolor=colors[1], marker=shapes[1], \n",
        "                       zorder=10, facecolors=second_face)\n",
        "            \n",
        "            first_pstr_len = len(get_p_stars(first_p))\n",
        "            second_pstr_len = len(get_p_stars(second_p))\n",
        "            p_str = np.array([' '] * 8)\n",
        "            if first_pstr_len > 0:\n",
        "                p_str[:first_pstr_len] = '*'\n",
        "            if second_pstr_len > 0:\n",
        "                p_str[-second_pstr_len:] = '⁺'\n",
        "            \n",
        "            feat_str = feat + '\\n' + ''.join(p_str)\n",
        "            yticklabels.append(feat_str)\n",
        "        except Exception as e:\n",
        "            yticklabels.append('')\n",
        "    \n",
        "    # add the axis labels\n",
        "    ax.set_xlabel('log-odds ratio', fontsize=14, family='serif')\n",
        "    ax.set_xticks([-xlim-.05, -.5, 0, .5, xlim])\n",
        "    ax.set_xticklabels(['on-track', -.5, 0, .5, 'awry'], fontsize=14, family='serif')\n",
        "    ax.set_yticks(yticks)\n",
        "    ax.set_yticklabels(yticklabels, fontsize=16, family='serif')\n",
        "    ax.tick_params(axis='both',  which='both', bottom='off',  top='off',left='off')\n",
        "    if title != '':\n",
        "        ax.text(0, (len(features) + 2.25)/ gap_prop, title, fontsize=title_size, family='serif',horizontalalignment='center',)\n",
        "    return feat_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPHaqEI1DCD"
      },
      "source": [
        "f, ax = plt.subplots(1,1, figsize=(5,10))\n",
        "_ = draw_figure(ax, first_comparisons, second_comparisons, 'First & second comment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGpeRshGJzaR"
      },
      "source": [
        "## Step 6: Construct feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhmFrYNz1HgL"
      },
      "source": [
        "def features_for_convo(convo_id, first_comment_id, second_comment_id):\n",
        "\n",
        "    # get prompt type features\n",
        "    try:\n",
        "        first_prompts = prompt_dist_df.loc[first_comment_id]\n",
        "    except:\n",
        "        first_prompts = pd.Series(data=np.ones(len(prompt_dist_df.columns)), index=prompt_dist_df.columns)\n",
        "    try:\n",
        "        second_prompts = prompt_dist_df.loc[second_comment_id].rename({c: c + \"_second\" for c in prompt_dist_df.columns})\n",
        "    except:\n",
        "        second_prompts = pd.Series(data=np.ones(len(prompt_dist_df.columns)), index=[c + \"_second\" for c in prompt_dist_df.columns])\n",
        "    prompts = first_prompts.append(second_prompts)\n",
        "    # get politeness strategies features\n",
        "    first_politeness = politeness_strategies.loc[first_comment_id]\n",
        "    second_politeness = politeness_strategies.loc[second_comment_id].rename({c: c + \"_second\" for c in politeness_strategies.columns})\n",
        "    politeness = first_politeness.append(second_politeness)\n",
        "    #return politeness.append(prompts)\n",
        "\n",
        "    ##################################################    \n",
        "    # get emotional strategies features - added by me!\n",
        "    temp_df = politeness.append(prompts)\n",
        "\n",
        "    first_emotional = emotion_df.loc[first_comment_id]\n",
        "    second_emotional = emotion_df.loc[second_comment_id].rename({c: c + \"_second\" for c in emotion_df.columns})\n",
        "    emotional = first_emotional.append(second_emotional)\n",
        "    \n",
        "    return emotional.append(temp_df)\n",
        "    ##################################################\n",
        "\n",
        "convo_ids = np.concatenate((pairs_df.conversation_id.values, pairs_df.bad_conversation_id.values))\n",
        "feats = [features_for_convo(row.conversation_id, row.first_id, row.second_id) for row in pairs_df.itertuples()] + \\\n",
        "        [features_for_convo(row.bad_conversation_id, row.bad_first_id, row.bad_second_id) for row in pairs_df.itertuples()]\n",
        "feature_table = pd.DataFrame(data=np.vstack([f.values for f in feats]), columns=feats[0].index, index=convo_ids)\n",
        "\n",
        "# in the paper, we dropped the sentiment lexicon based features (HASPOSITIVE and HASNEGATIVE), opting\n",
        "# to instead use them as a baseline. We do this here as well to be consistent with the paper.\n",
        "feature_table = feature_table.drop(columns=[\"feature_politeness_==HASPOSITIVE==\",\n",
        "                                            \"feature_politeness_==HASNEGATIVE==\",\n",
        "                                            \"feature_politeness_==HASPOSITIVE==_second\",\n",
        "                                            \"feature_politeness_==HASNEGATIVE==_second\"])\n",
        "\n",
        "feature_table.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFHaurDtKjyS"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl5EMtJqJsDS"
      },
      "source": [
        "## Step 7: Setting up prediction functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQu-rDJh1Sfe"
      },
      "source": [
        "def mode(seq):\n",
        "    vals, counts = np.unique(seq, return_counts=True)\n",
        "    return vals[np.argmax(counts)]\n",
        "\n",
        "def run_pred_single(inputs, X, y):\n",
        "    f_idx, (train_idx, test_idx) = inputs\n",
        "    \n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    \n",
        "    base_clf = Pipeline([(\"scaler\", StandardScaler()), (\"featselect\", SelectPercentile(f_classif, 10)), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
        "    clf = GridSearchCV(base_clf, {\"logreg__C\": [10**i for i in range(-4,4)], \"featselect__percentile\": list(range(10, 110, 10))}, cv=3)\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_scores = clf.predict_proba(X_test)[:,1]\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    feature_weights = clf.best_estimator_.named_steps[\"logreg\"].coef_.flatten()\n",
        "    feature_mask = clf.best_estimator_.named_steps[\"featselect\"].get_support()\n",
        "    \n",
        "    hyperparams = clf.best_params_\n",
        "    \n",
        "    return (y_pred, y_scores, feature_weights, hyperparams, feature_mask)\n",
        "\n",
        "def run_pred(X, y, fnames, groups):\n",
        "    feature_weights = {}\n",
        "    scores = np.asarray([np.nan for i in range(len(y))])\n",
        "    y_pred = np.zeros(len(y))\n",
        "    hyperparameters = defaultdict(list)\n",
        "    splits = list(enumerate(LeaveOneGroupOut().split(X, y, groups)))\n",
        "    accs = []\n",
        "        \n",
        "    with Pool(os.cpu_count()) as p:\n",
        "        prediction_results = p.map(partial(run_pred_single, X=X, y=y), splits)\n",
        "        \n",
        "    fselect_pvals_all = []\n",
        "    for i in range(len(splits)):\n",
        "        f_idx, (train_idx, test_idx) = splits[i]\n",
        "        y_pred_i, y_scores_i, weights_i, hyperparams_i, mask_i = prediction_results[i]\n",
        "        y_pred[test_idx] = y_pred_i\n",
        "        scores[test_idx] = y_scores_i\n",
        "        feature_weights[f_idx] = np.asarray([np.nan for _ in range(len(fnames))])\n",
        "        feature_weights[f_idx][mask_i] = weights_i\n",
        "        for param in hyperparams_i:\n",
        "            hyperparameters[param].append(hyperparams_i[param])   \n",
        "    \n",
        "    acc = np.mean(y_pred == y)\n",
        "    pvalue = stats.binom_test(sum(y_pred == y), n=len(y), alternative=\"greater\")\n",
        "                \n",
        "    coef_df = pd.DataFrame(feature_weights, index=fnames)\n",
        "    coef_df['mean_coef'] = coef_df.apply(np.nanmean, axis=1)\n",
        "    coef_df['std_coef'] = coef_df.apply(np.nanstd, axis=1)\n",
        "    return acc, coef_df[['mean_coef', 'std_coef']], scores, pd.DataFrame(hyperparameters), pvalue\n",
        "\n",
        "def get_labeled_pairs(pairs_df):\n",
        "    paired_labels = []\n",
        "    c0s = []\n",
        "    c1s = []\n",
        "    page_ids = []\n",
        "    for i, row in enumerate(pairs_df.itertuples()):\n",
        "        if i % 2 == 0:\n",
        "            c0s.append(row.conversation_id)\n",
        "            c1s.append(row.bad_conversation_id)\n",
        "        else:\n",
        "            c0s.append(row.bad_conversation_id)\n",
        "            c1s.append(row.conversation_id)\n",
        "        paired_labels.append(i%2)\n",
        "        page_ids.append(row.page_id)\n",
        "    return pd.DataFrame({\"c0\": c0s, \"c1\": c1s,\"first_convo_toxic\": paired_labels, \"page_id\": page_ids})\n",
        "\n",
        "def get_feature_subset(labeled_pairs_df, feature_list):\n",
        "    emotion_names = ['fear', 'anger', 'anticipation', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy', \n",
        "                    'fear_second', 'anger_second', 'anticipation_second', 'trust_second', 'surprise_second', 'positive_second', 'negative_second', 'sadness_second', 'disgust_second', 'joy_second']\n",
        "    prompt_type_names = [\"type_%d_dist\" % i for i in range(6)] + [\"type_%d_dist_second\" % i for i in range(6)]\n",
        "    politeness_names = [f for f in feature_table.columns if ((f not in prompt_type_names) and (f not in emotion_names))]\n",
        "    \n",
        "    #print(emotion_names)\n",
        "    #print(prompt_type_names)\n",
        "    #print(politeness_names)\n",
        "\n",
        "    features_to_use = []\n",
        "    if \"prompt_types\" in feature_list:\n",
        "        features_to_use += prompt_type_names\n",
        "    if \"politeness_strategies\" in feature_list:\n",
        "        features_to_use += politeness_names\n",
        "    if \"emotion_analytics\" in feature_list:\n",
        "        features_to_use += emotion_names\n",
        "    \n",
        "    feature_subset = feature_table[features_to_use]\n",
        "    \n",
        "    c0_feats = feature_subset.loc[labeled_pairs_df.c0].values\n",
        "    c1_feats = feature_subset.loc[labeled_pairs_df.c1].values\n",
        "\n",
        "    return c0_feats, c1_feats, features_to_use\n",
        "\n",
        "def run_pipeline(feature_set):\n",
        "    print(\"Running prediction task for feature set\", \"+\".join(feature_set))\n",
        "    print(\"Generating labels...\")\n",
        "    labeled_pairs_df = get_labeled_pairs(pairs_df)\n",
        "    print(\"Computing paired features...\")\n",
        "    X_c0, X_c1, feature_names = get_feature_subset(labeled_pairs_df, feature_set)\n",
        "    X = X_c1 - X_c0\n",
        "    print(\"Using\", X.shape[1], \"features\")\n",
        "    y = labeled_pairs_df.first_convo_toxic.values\n",
        "    print(\"Running leave-one-page-out prediction...\")\n",
        "    accuracy, coefs, scores, hyperparams, pvalue = run_pred(X, y, feature_names, labeled_pairs_df.page_id)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"p-value: %.4e\" % pvalue)\n",
        "    print(\"C (mode):\", mode(hyperparams.logreg__C))\n",
        "    print(\"Percent of features (mode):\", mode(hyperparams.featselect__percentile))\n",
        "    print(\"Coefficents:\")\n",
        "    print(coefs.sort_values(by=\"mean_coef\"))\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTbnlm25Jp7P"
      },
      "source": [
        "## Step 8: Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8yJ0DQj1X1v",
        "outputId": "d224564f-3421-4823-b398-5e3f2eb0fa55"
      },
      "source": [
        "#feature_combos = [[\"emotion_analytics\"],[\"politeness_strategies\"], [\"prompt_types\"], [\"politeness_strategies\", \"prompt_types\"]]\n",
        "#feature_combos = [[\"politeness_strategies\", \"emotion_analytics\"], [\"prompt_types\", \"emotion_analytics\"]]\n",
        "feature_combos = [[\"politeness_strategies\"], [\"prompt_types\"], [\"emotion_analytics\"], [\"politeness_strategies\", \"prompt_types\"], [\"politeness_strategies\", \"emotion_analytics\"], [\"prompt_types\", \"emotion_analytics\"], [\"politeness_strategies\", \"prompt_types\", \"emotion_analytics\"]]\n",
        "combo_names = []\n",
        "accs = []\n",
        "for combo in feature_combos:\n",
        "    combo_names.append(\"+\".join(combo).replace(\"_\", \" \"))\n",
        "    accuracy = run_pipeline(combo)\n",
        "    accs.append(accuracy)\n",
        "results_df = pd.DataFrame({\"Accuracy\": accs}, index=combo_names)\n",
        "results_df.index.name = \"Feature set\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running prediction task for feature set politeness_strategies\n",
            "Generating labels...\n",
            "Computing paired features...\n",
            "Using 38 features\n",
            "Running leave-one-page-out prediction...\n",
            "Accuracy: 0.583904109589041\n",
            "p-value: 2.8776e-05\n",
            "C (mode): 0.1\n",
            "Percent of features (mode): 40\n",
            "Coefficents:\n",
            "                                                   mean_coef  std_coef\n",
            "feature_politeness_==2nd_person==_second           -0.248812  0.106864\n",
            "feature_politeness_==2nd_person_start==            -0.201588  0.086540\n",
            "feature_politeness_==Direct_question==             -0.170186  0.071638\n",
            "feature_politeness_==2nd_person_start==_second     -0.165909  0.070901\n",
            "feature_politeness_==Direct_question==_second      -0.108820  0.046600\n",
            "feature_politeness_==Please_start==_second         -0.089840  0.038926\n",
            "feature_politeness_==Please==                      -0.083482  0.018584\n",
            "feature_politeness_==INDICATIVE==                  -0.024522  0.012429\n",
            "feature_politeness_==Hedges==_second               -0.005046  0.013304\n",
            "feature_politeness_==Factuality==                  -0.004573  0.002916\n",
            "feature_politeness_==1st_person==                   0.004219  0.002647\n",
            "feature_politeness_==Hedges==                       0.053870  0.021654\n",
            "feature_politeness_==Deference==_second             0.061232  0.009519\n",
            "feature_politeness_==1st_person_start==_second      0.066204  0.027506\n",
            "feature_politeness_==Apologizing==_second           0.090977  0.015798\n",
            "feature_politeness_==1st_person==_second            0.104571  0.044187\n",
            "feature_politeness_==INDICATIVE==_second            0.113841  0.060275\n",
            "feature_politeness_==HASHEDGE==                     0.116680  0.052291\n",
            "feature_politeness_==Indirect_(greeting)==          0.120899  0.053323\n",
            "feature_politeness_==Gratitude==                    0.137533  0.027814\n",
            "feature_politeness_==Gratitude==_second             0.137734  0.059117\n",
            "feature_politeness_==HASHEDGE==_second              0.198934  0.086719\n",
            "feature_politeness_==SUBJUNCTIVE==_second           0.225585  0.103825\n",
            "feature_politeness_==Please_start==                      NaN       NaN\n",
            "feature_politeness_==Indirect_(btw)==                    NaN       NaN\n",
            "feature_politeness_==Deference==                         NaN       NaN\n",
            "feature_politeness_==Apologizing==                       NaN       NaN\n",
            "feature_politeness_==1st_person_pl.==                    NaN       NaN\n",
            "feature_politeness_==1st_person_start==                  NaN       NaN\n",
            "feature_politeness_==2nd_person==                        NaN       NaN\n",
            "feature_politeness_==Direct_start==                      NaN       NaN\n",
            "feature_politeness_==SUBJUNCTIVE==                       NaN       NaN\n",
            "feature_politeness_==Please==_second                     NaN       NaN\n",
            "feature_politeness_==Indirect_(btw)==_second             NaN       NaN\n",
            "feature_politeness_==Factuality==_second                 NaN       NaN\n",
            "feature_politeness_==1st_person_pl.==_second             NaN       NaN\n",
            "feature_politeness_==Indirect_(greeting)==_second        NaN       NaN\n",
            "feature_politeness_==Direct_start==_second               NaN       NaN\n",
            "Running prediction task for feature set prompt_types\n",
            "Generating labels...\n",
            "Computing paired features...\n",
            "Using 12 features\n",
            "Running leave-one-page-out prediction...\n",
            "Accuracy: 0.4726027397260274\n",
            "p-value: 9.1399e-01\n",
            "C (mode): 0.0001\n",
            "Percent of features (mode): 10\n",
            "Coefficents:\n",
            "                    mean_coef  std_coef\n",
            "type_1_dist         -0.219329  0.202255\n",
            "type_5_dist         -0.157111  0.204781\n",
            "type_4_dist         -0.066120  0.058135\n",
            "type_0_dist         -0.048146  0.065061\n",
            "type_2_dist_second  -0.022530  0.019406\n",
            "type_4_dist_second  -0.011442  0.009933\n",
            "type_5_dist_second  -0.011111  0.010445\n",
            "type_1_dist_second  -0.001476  0.000000\n",
            "type_3_dist_second  -0.000188  0.000420\n",
            "type_0_dist_second   0.035375  0.030488\n",
            "type_3_dist          0.108458  0.150935\n",
            "type_2_dist          0.165865  0.223148\n",
            "Running prediction task for feature set emotion_analytics\n",
            "Generating labels...\n",
            "Computing paired features...\n",
            "Using 20 features\n",
            "Running leave-one-page-out prediction...\n",
            "Accuracy: 0.565068493150685\n",
            "p-value: 9.4381e-04\n",
            "C (mode): 1.0\n",
            "Percent of features (mode): 40\n",
            "Coefficents:\n",
            "                     mean_coef  std_coef\n",
            "anger                -0.152029  0.085331\n",
            "disgust              -0.115370  0.063332\n",
            "trust_second         -0.109253  0.000000\n",
            "fear_second          -0.081037  0.042561\n",
            "negative             -0.023073  0.013068\n",
            "joy_second           -0.002453  0.009145\n",
            "positive              0.025867  0.014939\n",
            "joy                   0.033032  0.010568\n",
            "surprise_second       0.120719  0.071103\n",
            "anticipation          0.123368  0.069919\n",
            "sadness_second        0.140214  0.026061\n",
            "positive_second       0.187920  0.106916\n",
            "fear                       NaN       NaN\n",
            "trust                      NaN       NaN\n",
            "surprise                   NaN       NaN\n",
            "sadness                    NaN       NaN\n",
            "anger_second               NaN       NaN\n",
            "anticipation_second        NaN       NaN\n",
            "negative_second            NaN       NaN\n",
            "disgust_second             NaN       NaN\n",
            "Running prediction task for feature set politeness_strategies+prompt_types\n",
            "Generating labels...\n",
            "Computing paired features...\n",
            "Using 50 features\n",
            "Running leave-one-page-out prediction...\n",
            "Accuracy: 0.601027397260274\n",
            "p-value: 5.9521e-07\n",
            "C (mode): 0.1\n",
            "Percent of features (mode): 40\n",
            "Coefficents:\n",
            "                                                   mean_coef  std_coef\n",
            "feature_politeness_==2nd_person==_second           -0.291935  0.040484\n",
            "feature_politeness_==2nd_person_start==            -0.244746  0.034102\n",
            "feature_politeness_==Direct_question==             -0.204984  0.027398\n",
            "feature_politeness_==2nd_person_start==_second     -0.195078  0.026829\n",
            "feature_politeness_==Direct_question==_second      -0.146072  0.019587\n",
            "feature_politeness_==Indirect_(btw)==              -0.106967  0.161316\n",
            "feature_politeness_==Please_start==_second         -0.102822  0.016270\n",
            "type_5_dist                                        -0.085695  0.072999\n",
            "feature_politeness_==Please==                      -0.083835  0.012576\n",
            "type_0_dist                                        -0.062414  0.013075\n",
            "feature_politeness_==Factuality==                  -0.050316  0.023122\n",
            "type_1_dist                                        -0.032249  0.060668\n",
            "feature_politeness_==INDICATIVE==                  -0.017545  0.006903\n",
            "feature_politeness_==Please==_second               -0.013106  0.005056\n",
            "type_2_dist_second                                 -0.006491  0.000000\n",
            "feature_politeness_==Hedges==_second               -0.005800  0.012892\n",
            "feature_politeness_==1st_person==                   0.004731  0.002573\n",
            "feature_politeness_==1st_person_start==             0.011695  0.004346\n",
            "feature_politeness_==1st_person_pl.==_second        0.028102  0.000000\n",
            "feature_politeness_==Deference==                    0.033094  0.000000\n",
            "feature_politeness_==Deference==_second             0.053484  0.019225\n",
            "feature_politeness_==Hedges==                       0.068143  0.012748\n",
            "feature_politeness_==Indirect_(btw)==_second        0.070587  0.000000\n",
            "feature_politeness_==1st_person_start==_second      0.074243  0.009590\n",
            "feature_politeness_==Apologizing==_second           0.089703  0.013760\n",
            "feature_politeness_==HASHEDGE==                     0.094569  0.019660\n",
            "feature_politeness_==INDICATIVE==_second            0.108543  0.047483\n",
            "feature_politeness_==1st_person==_second            0.109486  0.013937\n",
            "feature_politeness_==Gratitude==                    0.117582  0.017930\n",
            "feature_politeness_==Indirect_(greeting)==          0.118419  0.017581\n",
            "type_2_dist                                         0.161252  0.138562\n",
            "feature_politeness_==Gratitude==_second             0.170483  0.023965\n",
            "type_3_dist                                         0.198013  0.060159\n",
            "feature_politeness_==HASHEDGE==_second              0.237059  0.037461\n",
            "feature_politeness_==SUBJUNCTIVE==_second           0.255486  0.043615\n",
            "type_4_dist                                              NaN       NaN\n",
            "type_0_dist_second                                       NaN       NaN\n",
            "type_1_dist_second                                       NaN       NaN\n",
            "type_3_dist_second                                       NaN       NaN\n",
            "type_4_dist_second                                       NaN       NaN\n",
            "type_5_dist_second                                       NaN       NaN\n",
            "feature_politeness_==Please_start==                      NaN       NaN\n",
            "feature_politeness_==Apologizing==                       NaN       NaN\n",
            "feature_politeness_==1st_person_pl.==                    NaN       NaN\n",
            "feature_politeness_==2nd_person==                        NaN       NaN\n",
            "feature_politeness_==Direct_start==                      NaN       NaN\n",
            "feature_politeness_==SUBJUNCTIVE==                       NaN       NaN\n",
            "feature_politeness_==Factuality==_second                 NaN       NaN\n",
            "feature_politeness_==Indirect_(greeting)==_second        NaN       NaN\n",
            "feature_politeness_==Direct_start==_second               NaN       NaN\n",
            "Running prediction task for feature set politeness_strategies+emotion_analytics\n",
            "Generating labels...\n",
            "Computing paired features...\n",
            "Using 58 features\n",
            "Running leave-one-page-out prediction...\n",
            "Accuracy: 0.6164383561643836\n",
            "p-value: 1.0064e-08\n",
            "C (mode): 0.001\n",
            "Percent of features (mode): 70\n",
            "Coefficents:\n",
            "                                                   mean_coef      std_coef\n",
            "feature_politeness_==2nd_person==_second           -0.037196  2.756083e-02\n",
            "anger                                              -0.030320  1.879957e-02\n",
            "feature_politeness_==2nd_person_start==            -0.030016  2.105159e-02\n",
            "feature_politeness_==Direct_question==             -0.029969  1.963363e-02\n",
            "feature_politeness_==2nd_person_start==_second     -0.027157  2.023226e-02\n",
            "disgust                                            -0.026242  1.993904e-02\n",
            "feature_politeness_==Direct_question==_second      -0.020417  1.337978e-02\n",
            "fear_second                                        -0.018589  1.085126e-02\n",
            "feature_politeness_==Please_start==_second         -0.017360  1.163598e-02\n",
            "negative                                           -0.013648  6.503705e-03\n",
            "feature_politeness_==Please==                      -0.012812  5.042308e-03\n",
            "trust_second                                       -0.008843  3.279202e-03\n",
            "feature_politeness_==Factuality==                  -0.007607  2.541730e-03\n",
            "feature_politeness_==INDICATIVE==                  -0.007081  2.203031e-03\n",
            "feature_politeness_==Direct_start==_second         -0.006465  7.631988e-04\n",
            "feature_politeness_==Indirect_(btw)==              -0.005930  1.790572e-03\n",
            "sadness                                            -0.005876  1.838992e-03\n",
            "feature_politeness_==Please==_second               -0.005798  1.701912e-03\n",
            "feature_politeness_==2nd_person==                  -0.005034  1.452207e-03\n",
            "feature_politeness_==Please_start==                -0.004918  4.410019e-04\n",
            "disgust_second                                     -0.004563  0.000000e+00\n",
            "negative_second                                    -0.003377  1.126160e-03\n",
            "anger_second                                       -0.003174  1.087624e-03\n",
            "feature_politeness_==Deference==                    0.000746  2.911808e-07\n",
            "feature_politeness_==1st_person_pl.==_second        0.003835  1.487106e-03\n",
            "surprise                                            0.004287  0.000000e+00\n",
            "feature_politeness_==Indirect_(btw)==_second        0.004313  1.168435e-03\n",
            "anticipation_second                                 0.004429  2.234411e-04\n",
            "feature_politeness_==1st_person_start==             0.004968  1.408534e-03\n",
            "feature_politeness_==1st_person==                   0.005684  1.690514e-03\n",
            "feature_politeness_==Indirect_(greeting)==_second   0.006074  0.000000e+00\n",
            "feature_politeness_==Apologizing==                  0.007051  8.702795e-05\n",
            "joy_second                                          0.007324  2.165471e-03\n",
            "feature_politeness_==Deference==_second             0.007701  2.407523e-03\n",
            "sadness_second                                      0.009402  3.301743e-03\n",
            "feature_politeness_==INDICATIVE==_second            0.009967  4.035942e-03\n",
            "joy                                                 0.010739  3.338498e-03\n",
            "feature_politeness_==Apologizing==_second           0.011947  4.090283e-03\n",
            "feature_politeness_==Hedges==                       0.013637  6.294278e-03\n",
            "positive                                            0.013697  6.511119e-03\n",
            "feature_politeness_==Gratitude==                    0.014208  5.834996e-03\n",
            "feature_politeness_==Hedges==_second                0.015270  6.599056e-03\n",
            "feature_politeness_==Indirect_(greeting)==          0.016910  1.261061e-02\n",
            "feature_politeness_==1st_person_start==_second      0.017352  1.078389e-02\n",
            "surprise_second                                     0.018375  1.407427e-02\n",
            "feature_politeness_==1st_person==_second            0.019500  1.016540e-02\n",
            "feature_politeness_==HASHEDGE==                     0.019542  1.346892e-02\n",
            "feature_politeness_==Gratitude==_second             0.020416  1.293497e-02\n",
            "anticipation                                        0.022591  1.603897e-02\n",
            "feature_politeness_==SUBJUNCTIVE==_second           0.025950  2.144180e-02\n",
            "feature_politeness_==HASHEDGE==_second              0.029236  1.980606e-02\n",
            "positive_second                                     0.035449  2.499749e-02\n",
            "feature_politeness_==1st_person_pl.==                    NaN           NaN\n",
            "feature_politeness_==Direct_start==                      NaN           NaN\n",
            "feature_politeness_==SUBJUNCTIVE==                       NaN           NaN\n",
            "feature_politeness_==Factuality==_second                 NaN           NaN\n",
            "fear                                                     NaN           NaN\n",
            "trust                                                    NaN           NaN\n",
            "Running prediction task for feature set prompt_types+emotion_analytics\n",
            "Generating labels...\n",
            "Computing paired features...\n",
            "Using 32 features\n",
            "Running leave-one-page-out prediction...\n",
            "Accuracy: 0.5856164383561644\n",
            "p-value: 2.0131e-05\n",
            "C (mode): 0.1\n",
            "Percent of features (mode): 30\n",
            "Coefficents:\n",
            "                     mean_coef  std_coef\n",
            "anger                -0.195589  0.039413\n",
            "disgust              -0.158698  0.031316\n",
            "type_5_dist          -0.128773  0.027520\n",
            "fear_second          -0.103308  0.018397\n",
            "negative             -0.029245  0.006900\n",
            "type_0_dist          -0.026371  0.014333\n",
            "joy_second           -0.006920  0.009109\n",
            "trust_second         -0.005276  0.003479\n",
            "type_4_dist_second   -0.004586  0.000000\n",
            "sadness              -0.003191  0.001924\n",
            "negative_second      -0.003121  0.000000\n",
            "type_2_dist_second   -0.002220  0.001278\n",
            "anger_second         -0.002009  0.001309\n",
            "disgust_second       -0.000421  0.000000\n",
            "anticipation_second   0.000459  0.000000\n",
            "type_1_dist           0.004090  0.002493\n",
            "sadness_second        0.006290  0.004182\n",
            "positive              0.017156  0.006875\n",
            "joy                   0.028021  0.005236\n",
            "type_2_dist           0.110129  0.033043\n",
            "anticipation          0.160151  0.032400\n",
            "surprise_second       0.170820  0.037134\n",
            "type_3_dist           0.198445  0.054771\n",
            "positive_second       0.242062  0.048273\n",
            "type_4_dist                NaN       NaN\n",
            "type_0_dist_second         NaN       NaN\n",
            "type_1_dist_second         NaN       NaN\n",
            "type_3_dist_second         NaN       NaN\n",
            "type_5_dist_second         NaN       NaN\n",
            "fear                       NaN       NaN\n",
            "trust                      NaN       NaN\n",
            "surprise                   NaN       NaN\n",
            "Running prediction task for feature set politeness_strategies+prompt_types+emotion_analytics\n",
            "Generating labels...\n",
            "Computing paired features...\n",
            "Using 70 features\n",
            "Running leave-one-page-out prediction...\n",
            "Accuracy: 0.6044520547945206\n",
            "p-value: 2.5239e-07\n",
            "C (mode): 0.001\n",
            "Percent of features (mode): 70\n",
            "Coefficents:\n",
            "                                                mean_coef  std_coef\n",
            "feature_politeness_==2nd_person==_second        -0.088623  0.092533\n",
            "feature_politeness_==2nd_person_start==         -0.071498  0.073646\n",
            "disgust                                         -0.069589  0.078879\n",
            "feature_politeness_==2nd_person_start==_second  -0.066542  0.070820\n",
            "feature_politeness_==Direct_question==          -0.065209  0.061674\n",
            "...                                                   ...       ...\n",
            "anticipation                                     0.052561  0.053876\n",
            "type_3_dist                                      0.063761  0.072363\n",
            "feature_politeness_==HASHEDGE==_second           0.066755  0.067721\n",
            "feature_politeness_==SUBJUNCTIVE==_second        0.070682  0.084196\n",
            "positive_second                                  0.082464  0.084921\n",
            "\n",
            "[70 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "EuUVn-2GnyVQ",
        "outputId": "d7a0ae07-ca29-4a3b-bf7f-83fff2886c1f"
      },
      "source": [
        "results_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feature set</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>politeness strategies</th>\n",
              "      <td>0.583904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prompt types</th>\n",
              "      <td>0.472603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion analytics</th>\n",
              "      <td>0.565068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>politeness strategies+prompt types</th>\n",
              "      <td>0.601027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>politeness strategies+emotion analytics</th>\n",
              "      <td>0.616438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prompt types+emotion analytics</th>\n",
              "      <td>0.585616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>politeness strategies+prompt types+emotion analytics</th>\n",
              "      <td>0.604452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Accuracy\n",
              "Feature set                                                 \n",
              "politeness strategies                               0.583904\n",
              "prompt types                                        0.472603\n",
              "emotion analytics                                   0.565068\n",
              "politeness strategies+prompt types                  0.601027\n",
              "politeness strategies+emotion analytics             0.616438\n",
              "prompt types+emotion analytics                      0.585616\n",
              "politeness strategies+prompt types+emotion anal...  0.604452"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}