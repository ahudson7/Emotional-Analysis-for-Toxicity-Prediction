{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvosGoneAwry.ipynb",
      "provenance": [],
      "mount_file_id": "114N2PtJ-C2UtuX4JM1N-JakEZTOgalBp",
      "authorship_tag": "ABX9TyPpV4YPv6Cz2ijBCZqRTDsh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahudson7/Emotional-Analysis-for-Toxicity-Prediction/blob/main/ConvosGoneAwry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ctQc0CLbnN"
      },
      "source": [
        "The code in this notebook is modified from the Cornell Conversational Analysis Toolkit (aka: Convokit), which can be found at https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb.  \n",
        "\n",
        "The paper associated with this work is:\n",
        "\n",
        "Justine Zhang, Jonathan Chang, Cristian Danescu-Niculescu-Mizil, Lucas Dixon, Yiqing Hua, Dario Taraborelli, and Nithum Thain. 2018. Conversations gone awry: Detecting early signs of conversational failure. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1350–1361, Melbourne, Australia. Association for Computational Linguistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnfWvKvtJNS-"
      },
      "source": [
        "#Pre-processing Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCu7vl-0KbN8"
      },
      "source": [
        "## Step 0: Setting up workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrHvRcBeG-Ot"
      },
      "source": [
        "pip install NRCLex # not necessary since importing pre-done CSV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w_whGB2gYys"
      },
      "source": [
        "pip install convokit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HViZ40yUcMdC"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut\n",
        "from sklearn.feature_selection import f_classif, SelectPercentile\n",
        "\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from convokit import download\n",
        "from convokit.prompt_types import PromptTypeWrapper\n",
        "from convokit import PolitenessStrategies\n",
        "from convokit import Corpus\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snj_mI_FKT08"
      },
      "source": [
        "## Step 1: Loading the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2t6ofEdhr7N"
      },
      "source": [
        "# OPTION 1: DOWNLOAD CORPUS \n",
        "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
        "# DATA_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing'\n",
        "# AWRY_ROOT_DIR = download('conversations-gone-awry-corpus', data_dir=DATA_DIR)\n",
        "# AWRY_ROOT_DIR = download('conversations-gone-awry-cmv-corpus', data_dir=DATA_DIR)\n",
        "\n",
        "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
        "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE TENNIS-CORPUS IS LOCATED\n",
        "# AWRY_ROOT_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/conversations-gone-awry-corpus'\n",
        "AWRY_ROOT_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/conversations-gone-awry-cmv-corpus'\n",
        "\n",
        "awry_corpus = Corpus(AWRY_ROOT_DIR)\n",
        "awry_corpus.load_info('utterance',['parsed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLO5XBXqhkJh"
      },
      "source": [
        "awry_corpus = awry_corpus.filter_conversations_by(lambda convo: convo.meta['annotation_year'] == '2018')\n",
        "awry_corpus.print_summary_stats()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKWTGWhfLRiB"
      },
      "source": [
        "# Feature development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqRgwImBKWcq"
      },
      "source": [
        "## Step 2: Extract prompt types features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgKiI0j6ho6A"
      },
      "source": [
        "# OPTION 1: DOWNLOAD CORPUS \n",
        "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
        "# DATA_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing'\n",
        "# FULL_ROOT_DIR = download('tennis-corpus', data_dir=DATA_DIR)\n",
        "\n",
        "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
        "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE TENNIS-CORPUS IS LOCATED\n",
        "FULL_ROOT_DIR = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/tennis-corpus'\n",
        "\n",
        "full_corpus = Corpus(FULL_ROOT_DIR)\n",
        "full_corpus.load_info('utterance',['parsed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq7erLFyM6km"
      },
      "source": [
        "# We encountered an issue with this package that occurs due to a known bug. The best workaround we found was to import and install it right it is needed in the next step.\n",
        "# Sometimes it still runs into errors, and reloading the workbook and then retrying will often fix them.\n",
        "\n",
        "import spacy \n",
        "\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NnfkwXDyTaL"
      },
      "source": [
        "pt_model = PromptTypeWrapper(n_types=6, use_prompt_motifs=False, root_only=False,\n",
        "                            questions_only=False, enforce_caps=False, min_support=20, min_df=100,\n",
        "                            svd__n_components=50, max_dist=1., random_state=1000)\n",
        "pt_model.fit(full_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyslEUlOz7Kg"
      },
      "source": [
        "pt_model.summarize(full_corpus, k=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKsRyDyM0IqD"
      },
      "source": [
        "TYPE_NAMES = ['Prompt: Casual', 'Prompt: Moderation', 'Prompt: Coordination', 'Prompt: Contention',\n",
        "             'Prompt: Editing', 'Prompt: Procedures']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edmQceN80KSH"
      },
      "source": [
        "awry_corpus = pt_model.transform(awry_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "537FQFTx0OCm"
      },
      "source": [
        "prompt_dist_df = awry_corpus.get_vectors(name='prompt_types__prompt_dists.6', \n",
        "                                         as_dataframe=True)\n",
        "type_ids = np.argmin(prompt_dist_df.values, axis=1)\n",
        "mask  = np.min(prompt_dist_df.values, axis=1) >= 1.\n",
        "type_ids[mask] = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNKLc04M0RKe"
      },
      "source": [
        "prompt_dist_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633hXD9J0UYn"
      },
      "source": [
        "prompt_type_assignments = np.zeros((len(prompt_dist_df), prompt_dist_df.shape[1]+1))\n",
        "prompt_type_assignments[np.arange(len(type_ids)),type_ids] = 1\n",
        "prompt_type_assignment_df = pd.DataFrame(columns=np.arange(prompt_dist_df.shape[1]+1), index=prompt_dist_df.index, \n",
        "                                        data=prompt_type_assignments)\n",
        "prompt_type_assignment_df = prompt_type_assignment_df[prompt_type_assignment_df.columns[:-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inwBWkgs0W2a"
      },
      "source": [
        "prompt_type_assignment_df.columns = TYPE_NAMES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy4RUnBT0YS6"
      },
      "source": [
        "prompt_type_assignment_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdDwTxsQKFqd"
      },
      "source": [
        "## Step 3: Extract politeness strategies features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaWyXT3O0b3L"
      },
      "source": [
        "\n",
        "\n",
        "ps = PolitenessStrategies(verbose=1000)\n",
        "awry_corpus = ps.transform(awry_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5WXoaw0hEG"
      },
      "source": [
        "utterance_ids = awry_corpus.get_utterance_ids()\n",
        "rows = []\n",
        "for uid in utterance_ids:\n",
        "    rows.append(awry_corpus.get_utterance(uid).meta[\"politeness_strategies\"])\n",
        "politeness_strategies = pd.DataFrame(rows, index=utterance_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc2tMyQJ0kO4"
      },
      "source": [
        "politeness_strategies.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pMrLuo5KCLt"
      },
      "source": [
        "## Step 3.5: Extracting emotional information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dE5UwcqGkqr"
      },
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CS510 - Adventures in NLP/Project/Notebook Testing/CGA_emotion_df.csv'\n",
        "emotion_df = pd.read_csv(file_path, index_col = 0)\n",
        "\n",
        "'''\n",
        "from nrclex import NRCLex\n",
        "#from convokit import Corpus, download\n",
        "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
        "\n",
        "count = 0\n",
        "\n",
        "emotion_df = pd.DataFrame(columns=['fear', 'anger', 'anticipation', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy'])\n",
        "\n",
        "emotion_list = ['fear', 'anger', 'anticipation', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy']\n",
        "\n",
        "for utt in corpus.iter_utterances():\n",
        "    emotion = NRCLex(utt.text)\n",
        "    temp = []\n",
        "    for k in emotion.affect_frequencies.keys():\n",
        "        if k in emotion_list:\n",
        "            emotion_df.loc[utt.id, k] = emotion.affect_frequencies[k]\n",
        "        elif k == 'anticip':\n",
        "            emotion_df.loc[utt.id, 'anticipation'] = emotion.affect_frequencies[k]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoBFRbg4m2TK"
      },
      "source": [
        "emotion_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrZsMd8H4Iy"
      },
      "source": [
        "## Step 4: Create pair data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G0si-jR0m1N"
      },
      "source": [
        "# first, we need to directly map comment IDs to their conversations. We'll build a DataFrame to do this\n",
        "comment_ids = []\n",
        "convo_ids = []\n",
        "timestamps = []\n",
        "page_ids = []\n",
        "for conversation in awry_corpus.iter_conversations():\n",
        "    for comment in conversation.iter_utterances():\n",
        "        # section headers are included in the dataset for completeness, but for prediction we need to ignore\n",
        "        # them as they are not utterances\n",
        "        if not comment.meta[\"is_section_header\"]:\n",
        "            comment_ids.append(comment.id)\n",
        "            convo_ids.append(comment.root)\n",
        "            timestamps.append(comment.timestamp)\n",
        "            page_ids.append(conversation.meta[\"page_id\"])\n",
        "comment_df = pd.DataFrame({\"conversation_id\": convo_ids, \"timestamp\": timestamps, \"page_id\": page_ids}, index=comment_ids)\n",
        "\n",
        "# we'll do our construction using awry conversation ID's as the reference key\n",
        "awry_convo_ids = set()\n",
        "# these dicts will then all be keyed by awry ID\n",
        "good_convo_map = {}\n",
        "page_id_map = {}\n",
        "for conversation in awry_corpus.iter_conversations():\n",
        "    if conversation.meta[\"conversation_has_personal_attack\"] and conversation.id not in awry_convo_ids:\n",
        "        awry_convo_ids.add(conversation.id)\n",
        "        good_convo_map[conversation.id] = conversation.meta[\"pair_id\"]\n",
        "        page_id_map[conversation.id] = conversation.meta[\"page_id\"]\n",
        "awry_convo_ids = list(awry_convo_ids)\n",
        "pairs_df = pd.DataFrame({\"bad_conversation_id\": awry_convo_ids,\n",
        "                         \"conversation_id\": [good_convo_map[cid] for cid in awry_convo_ids],\n",
        "                         \"page_id\": [page_id_map[cid] for cid in awry_convo_ids]})\n",
        "# finally, we will augment the pairs dataframe with the IDs of the first and second comment for both\n",
        "# the bad and good conversation. This will come in handy for constructing the feature matrix.\n",
        "first_ids = []\n",
        "second_ids = []\n",
        "first_ids_bad = []\n",
        "second_ids_bad = []\n",
        "for row in pairs_df.itertuples():\n",
        "    # \"first two\" is defined in terms of time of posting\n",
        "    comments_sorted = comment_df[comment_df.conversation_id==row.conversation_id].sort_values(by=\"timestamp\")\n",
        "    first_ids.append(comments_sorted.iloc[0].name)\n",
        "    second_ids.append(comments_sorted.iloc[1].name)\n",
        "    comments_sorted_bad = comment_df[comment_df.conversation_id==row.bad_conversation_id].sort_values(by=\"timestamp\")\n",
        "    first_ids_bad.append(comments_sorted_bad.iloc[0].name)\n",
        "    second_ids_bad.append(comments_sorted_bad.iloc[1].name)\n",
        "pairs_df = pairs_df.assign(first_id=first_ids, second_id=second_ids, \n",
        "                           bad_first_id=first_ids_bad, bad_second_id=second_ids_bad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQlezI96J8Zl"
      },
      "source": [
        "## Step 5: Comparing features exhibited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7clBI4p10sjD"
      },
      "source": [
        "def clean_feature_name(feat):\n",
        "    new_feat = feat.replace('feature_politeness','').replace('==','').replace('_', ' ')\n",
        "    split = new_feat.split()\n",
        "    first, rest = split[0], ' '.join(split[1:]).lower()\n",
        "    if first[0].isalpha():\n",
        "        first = first.title()\n",
        "    if 'Hashedge' in first:\n",
        "        return 'Hedge (lexicon)'\n",
        "    if 'Hedges' in first:\n",
        "        return 'Hedge (dep. tree)'\n",
        "    if 'greeting' in feat:\n",
        "        return 'Greetings'\n",
        "    cleaner_str = first + ' ' + rest\n",
        "#     cleaner_str = cleaner_str.replace('2nd', '2$\\mathregular{^{nd}}$').replace('1st', '1$\\mathregular{^{st}}$')\n",
        "    return cleaner_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeX3unSb0yGz"
      },
      "source": [
        "politeness_strategies_display = politeness_strategies[[col for col in politeness_strategies.columns \n",
        "                  if col not in ['feature_politeness_==HASNEGATIVE==', 'feature_politeness_==HASPOSITIVE==']]].copy()\n",
        "politeness_strategies_display.columns = [clean_feature_name(col) for col in politeness_strategies_display.columns]\n",
        "\n",
        "all_features = politeness_strategies_display.join(prompt_type_assignment_df)\n",
        "\n",
        "tox_first_comment_features =pairs_df[['bad_first_id']].join(all_features, how='left', on='bad_first_id')[all_features.columns]\n",
        "ntox_first_comment_features =pairs_df[['first_id']].join(all_features, how='left', on='first_id')[all_features.columns]\n",
        "\n",
        "tox_second_comment_features =pairs_df[['bad_second_id']].join(all_features, how='left', on='bad_second_id')[all_features.columns]\n",
        "ntox_second_comment_features =pairs_df[['second_id']].join(all_features, how='left', on='second_id')[all_features.columns]\n",
        "\n",
        "\n",
        "def get_p_stars(x):\n",
        "    if x < .001: return '***'\n",
        "    elif x < .01: return '**'\n",
        "    elif x < .05: return '*'\n",
        "    else: return ''\n",
        "def compare_tox(df_ntox, df_tox,  min_n=0):\n",
        "    cols = df_ntox.columns\n",
        "    num_feats_in_tox = df_tox[cols].sum().astype(int).rename('num_feat_tox')\n",
        "    num_nfeats_in_tox = (1 - df_tox[cols]).sum().astype(int).rename('num_nfeat_tox')\n",
        "    num_feats_in_ntox = df_ntox[cols].sum().astype(int).rename('num_feat_ntox')\n",
        "    num_nfeats_in_ntox = (1 - df_ntox[cols]).sum().astype(int).rename('num_nfeat_ntox')\n",
        "    prop_tox = df_tox[cols].mean().rename('prop_tox')\n",
        "    ref_prop_ntox = df_ntox[cols].mean().rename('prop_ntox')\n",
        "    n_tox = len(df_tox)\n",
        "    df = pd.concat([\n",
        "        num_feats_in_tox, \n",
        "        num_nfeats_in_tox,\n",
        "        num_feats_in_ntox,\n",
        "        num_nfeats_in_ntox,\n",
        "        prop_tox,\n",
        "        ref_prop_ntox,\n",
        "    ], axis=1)\n",
        "    df['num_total'] = df.num_feat_tox + df.num_feat_ntox\n",
        "    df['log_odds'] = np.log(df.num_feat_tox) - np.log(df.num_nfeat_tox) \\\n",
        "        + np.log(df.num_nfeat_ntox) - np.log(df.num_feat_ntox)\n",
        "    df['abs_log_odds'] = np.abs(df.log_odds)\n",
        "    df['binom_p'] = df.apply(lambda x: stats.binom_test(x.num_feat_tox, n_tox, x.prop_ntox), axis=1)\n",
        "    df = df[df.num_total >= min_n]\n",
        "    df['p'] = df['binom_p'].apply(lambda x: '%.3f' % x)\n",
        "    df['pstars'] = df['binom_p'].apply(get_p_stars)\n",
        "    return df.sort_values('log_odds', ascending=False)\n",
        "\n",
        "first_comparisons = compare_tox(ntox_first_comment_features, tox_first_comment_features)\n",
        "second_comparisons = compare_tox(ntox_second_comment_features, tox_second_comment_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc1c40NY0-wj"
      },
      "source": [
        "# we are now ready to plot these comparisons. the following (rather intimidating) helper function \n",
        "# produces a nicely-formatted plot:\n",
        "def draw_figure(ax, first_cmp, second_cmp, title='', prompt_types=6, min_log_odds=.2, min_freq=50,xlim=.85):\n",
        "\n",
        "    # selecting and sorting the features to plot, given minimum effect sizes and statistical significance\n",
        "    frequent_feats = first_cmp[first_cmp.num_total >= min_freq].index.union(second_cmp[second_cmp.num_total >= min_freq].index)\n",
        "    lrg_effect_feats = first_cmp[(first_cmp.abs_log_odds >= .2)\n",
        "                                & (first_cmp.binom_p < .05)].index.union(second_cmp[(second_cmp.abs_log_odds >= .2)\n",
        "                                                                                  & (second_cmp.binom_p < .05)].index)\n",
        "    feats_to_include = frequent_feats.intersection(lrg_effect_feats)\n",
        "    feat_order = sorted(feats_to_include, key=lambda x: first_cmp.loc[x].log_odds, reverse=True)\n",
        "\n",
        "    # parameters determining the look of the figure\n",
        "    colors = ['darkorchid', 'seagreen']\n",
        "    shapes = ['d', 's']    \n",
        "    eps = .02\n",
        "    star_eps = .035\n",
        "    xlim = xlim\n",
        "    min_log = .2\n",
        "    gap_prop = 2\n",
        "    label_size = 14\n",
        "    title_size=18\n",
        "    radius = 144\n",
        "    features = feat_order\n",
        "    ax.invert_yaxis()\n",
        "    ax.plot([0,0], [0, len(features)/gap_prop], color='black')\n",
        "    \n",
        "    # for each figure we plot the point according to effect size in the first and second comment, \n",
        "    # and add axis labels denoting statistical significance\n",
        "    yticks = []\n",
        "    yticklabels = []\n",
        "    for f_idx, feat in enumerate(features):\n",
        "        curr_y = (f_idx + .5)/gap_prop\n",
        "        yticks.append(curr_y)\n",
        "        try:\n",
        "            \n",
        "            first_p = first_cmp.loc[feat].binom_p\n",
        "            second_p = second_cmp.loc[feat].binom_p            \n",
        "            if first_cmp.loc[feat].abs_log_odds < min_log:\n",
        "                first_face = \"white\"\n",
        "            elif first_p >= 0.05:\n",
        "                first_face = 'white'\n",
        "            else:\n",
        "                first_face = colors[0]\n",
        "            if second_cmp.loc[feat].abs_log_odds < min_log:\n",
        "                second_face = \"white\"\n",
        "            elif second_p >= 0.05:\n",
        "                second_face = 'white'\n",
        "            else:\n",
        "                second_face = colors[1]\n",
        "            ax.plot([-1 * xlim, xlim], [curr_y, curr_y], '--', color='grey', zorder=0, linewidth=.5)\n",
        "            \n",
        "            ax.scatter([first_cmp.loc[feat].log_odds], [curr_y + eps], s=radius, edgecolor=colors[0], marker=shapes[0],\n",
        "                        zorder=20, facecolors=first_face)\n",
        "            ax.scatter([second_cmp.loc[feat].log_odds], [curr_y + eps], s=radius, edgecolor=colors[1], marker=shapes[1], \n",
        "                       zorder=10, facecolors=second_face)\n",
        "            \n",
        "            first_pstr_len = len(get_p_stars(first_p))\n",
        "            second_pstr_len = len(get_p_stars(second_p))\n",
        "            p_str = np.array([' '] * 8)\n",
        "            if first_pstr_len > 0:\n",
        "                p_str[:first_pstr_len] = '*'\n",
        "            if second_pstr_len > 0:\n",
        "                p_str[-second_pstr_len:] = '⁺'\n",
        "            \n",
        "            feat_str = feat + '\\n' + ''.join(p_str)\n",
        "            yticklabels.append(feat_str)\n",
        "        except Exception as e:\n",
        "            yticklabels.append('')\n",
        "    \n",
        "    # add the axis labels\n",
        "    ax.set_xlabel('log-odds ratio', fontsize=14, family='serif')\n",
        "    ax.set_xticks([-xlim-.05, -.5, 0, .5, xlim])\n",
        "    ax.set_xticklabels(['on-track', -.5, 0, .5, 'awry'], fontsize=14, family='serif')\n",
        "    ax.set_yticks(yticks)\n",
        "    ax.set_yticklabels(yticklabels, fontsize=16, family='serif')\n",
        "    ax.tick_params(axis='both',  which='both', bottom='off',  top='off',left='off')\n",
        "    if title != '':\n",
        "        ax.text(0, (len(features) + 2.25)/ gap_prop, title, fontsize=title_size, family='serif',horizontalalignment='center',)\n",
        "    return feat_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPHaqEI1DCD"
      },
      "source": [
        "f, ax = plt.subplots(1,1, figsize=(5,10))\n",
        "_ = draw_figure(ax, first_comparisons, second_comparisons, 'First & second comment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGpeRshGJzaR"
      },
      "source": [
        "## Step 6: Construct feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhmFrYNz1HgL"
      },
      "source": [
        "def features_for_convo(convo_id, first_comment_id, second_comment_id):\n",
        "\n",
        "    # get prompt type features\n",
        "    try:\n",
        "        first_prompts = prompt_dist_df.loc[first_comment_id]\n",
        "    except:\n",
        "        first_prompts = pd.Series(data=np.ones(len(prompt_dist_df.columns)), index=prompt_dist_df.columns)\n",
        "    try:\n",
        "        second_prompts = prompt_dist_df.loc[second_comment_id].rename({c: c + \"_second\" for c in prompt_dist_df.columns})\n",
        "    except:\n",
        "        second_prompts = pd.Series(data=np.ones(len(prompt_dist_df.columns)), index=[c + \"_second\" for c in prompt_dist_df.columns])\n",
        "    prompts = first_prompts.append(second_prompts)\n",
        "    # get politeness strategies features\n",
        "    first_politeness = politeness_strategies.loc[first_comment_id]\n",
        "    second_politeness = politeness_strategies.loc[second_comment_id].rename({c: c + \"_second\" for c in politeness_strategies.columns})\n",
        "    politeness = first_politeness.append(second_politeness)\n",
        "    #return politeness.append(prompts)\n",
        "\n",
        "    ##################################################    \n",
        "    # get emotional strategies features - added by me!\n",
        "    temp_df = politeness.append(prompts)\n",
        "\n",
        "    first_emotional = emotion_df.loc[first_comment_id]\n",
        "    second_emotional = emotion_df.loc[second_comment_id].rename({c: c + \"_second\" for c in emotion_df.columns})\n",
        "    emotional = first_emotional.append(second_emotional)\n",
        "    \n",
        "    return emotional.append(temp_df)\n",
        "    ##################################################\n",
        "\n",
        "convo_ids = np.concatenate((pairs_df.conversation_id.values, pairs_df.bad_conversation_id.values))\n",
        "feats = [features_for_convo(row.conversation_id, row.first_id, row.second_id) for row in pairs_df.itertuples()] + \\\n",
        "        [features_for_convo(row.bad_conversation_id, row.bad_first_id, row.bad_second_id) for row in pairs_df.itertuples()]\n",
        "feature_table = pd.DataFrame(data=np.vstack([f.values for f in feats]), columns=feats[0].index, index=convo_ids)\n",
        "\n",
        "# in the paper, we dropped the sentiment lexicon based features (HASPOSITIVE and HASNEGATIVE), opting\n",
        "# to instead use them as a baseline. We do this here as well to be consistent with the paper.\n",
        "feature_table = feature_table.drop(columns=[\"feature_politeness_==HASPOSITIVE==\",\n",
        "                                            \"feature_politeness_==HASNEGATIVE==\",\n",
        "                                            \"feature_politeness_==HASPOSITIVE==_second\",\n",
        "                                            \"feature_politeness_==HASNEGATIVE==_second\"])\n",
        "\n",
        "feature_table.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFHaurDtKjyS"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl5EMtJqJsDS"
      },
      "source": [
        "## Step 7: Setting up prediction functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQu-rDJh1Sfe"
      },
      "source": [
        "def mode(seq):\n",
        "    vals, counts = np.unique(seq, return_counts=True)\n",
        "    return vals[np.argmax(counts)]\n",
        "\n",
        "def run_pred_single(inputs, X, y):\n",
        "    f_idx, (train_idx, test_idx) = inputs\n",
        "    \n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    \n",
        "    base_clf = Pipeline([(\"scaler\", StandardScaler()), (\"featselect\", SelectPercentile(f_classif, 10)), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
        "    clf = GridSearchCV(base_clf, {\"logreg__C\": [10**i for i in range(-4,4)], \"featselect__percentile\": list(range(10, 110, 10))}, cv=3)\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_scores = clf.predict_proba(X_test)[:,1]\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    feature_weights = clf.best_estimator_.named_steps[\"logreg\"].coef_.flatten()\n",
        "    feature_mask = clf.best_estimator_.named_steps[\"featselect\"].get_support()\n",
        "    \n",
        "    hyperparams = clf.best_params_\n",
        "    \n",
        "    return (y_pred, y_scores, feature_weights, hyperparams, feature_mask)\n",
        "\n",
        "def run_pred(X, y, fnames, groups):\n",
        "    feature_weights = {}\n",
        "    scores = np.asarray([np.nan for i in range(len(y))])\n",
        "    y_pred = np.zeros(len(y))\n",
        "    hyperparameters = defaultdict(list)\n",
        "    splits = list(enumerate(LeaveOneGroupOut().split(X, y, groups)))\n",
        "    accs = []\n",
        "        \n",
        "    with Pool(os.cpu_count()) as p:\n",
        "        prediction_results = p.map(partial(run_pred_single, X=X, y=y), splits)\n",
        "        \n",
        "    fselect_pvals_all = []\n",
        "    for i in range(len(splits)):\n",
        "        f_idx, (train_idx, test_idx) = splits[i]\n",
        "        y_pred_i, y_scores_i, weights_i, hyperparams_i, mask_i = prediction_results[i]\n",
        "        y_pred[test_idx] = y_pred_i\n",
        "        scores[test_idx] = y_scores_i\n",
        "        feature_weights[f_idx] = np.asarray([np.nan for _ in range(len(fnames))])\n",
        "        feature_weights[f_idx][mask_i] = weights_i\n",
        "        for param in hyperparams_i:\n",
        "            hyperparameters[param].append(hyperparams_i[param])   \n",
        "    \n",
        "    acc = np.mean(y_pred == y)\n",
        "    pvalue = stats.binom_test(sum(y_pred == y), n=len(y), alternative=\"greater\")\n",
        "                \n",
        "    coef_df = pd.DataFrame(feature_weights, index=fnames)\n",
        "    coef_df['mean_coef'] = coef_df.apply(np.nanmean, axis=1)\n",
        "    coef_df['std_coef'] = coef_df.apply(np.nanstd, axis=1)\n",
        "    return acc, coef_df[['mean_coef', 'std_coef']], scores, pd.DataFrame(hyperparameters), pvalue\n",
        "\n",
        "def get_labeled_pairs(pairs_df):\n",
        "    paired_labels = []\n",
        "    c0s = []\n",
        "    c1s = []\n",
        "    page_ids = []\n",
        "    for i, row in enumerate(pairs_df.itertuples()):\n",
        "        if i % 2 == 0:\n",
        "            c0s.append(row.conversation_id)\n",
        "            c1s.append(row.bad_conversation_id)\n",
        "        else:\n",
        "            c0s.append(row.bad_conversation_id)\n",
        "            c1s.append(row.conversation_id)\n",
        "        paired_labels.append(i%2)\n",
        "        page_ids.append(row.page_id)\n",
        "    return pd.DataFrame({\"c0\": c0s, \"c1\": c1s,\"first_convo_toxic\": paired_labels, \"page_id\": page_ids})\n",
        "\n",
        "def get_feature_subset(labeled_pairs_df, feature_list):\n",
        "    emotion_names = ['fear', 'anger', 'anticipation', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy', \n",
        "                    'fear_second', 'anger_second', 'anticipation_second', 'trust_second', 'surprise_second', 'positive_second', 'negative_second', 'sadness_second', 'disgust_second', 'joy_second']\n",
        "    prompt_type_names = [\"type_%d_dist\" % i for i in range(6)] + [\"type_%d_dist_second\" % i for i in range(6)]\n",
        "    politeness_names = [f for f in feature_table.columns if ((f not in prompt_type_names) and (f not in emotion_names))]\n",
        "    \n",
        "    #print(emotion_names)\n",
        "    #print(prompt_type_names)\n",
        "    #print(politeness_names)\n",
        "\n",
        "    features_to_use = []\n",
        "    if \"prompt_types\" in feature_list:\n",
        "        features_to_use += prompt_type_names\n",
        "    if \"politeness_strategies\" in feature_list:\n",
        "        features_to_use += politeness_names\n",
        "    if \"emotion_analytics\" in feature_list:\n",
        "        features_to_use += emotion_names\n",
        "    \n",
        "    feature_subset = feature_table[features_to_use]\n",
        "    \n",
        "    c0_feats = feature_subset.loc[labeled_pairs_df.c0].values\n",
        "    c1_feats = feature_subset.loc[labeled_pairs_df.c1].values\n",
        "\n",
        "    return c0_feats, c1_feats, features_to_use\n",
        "\n",
        "def run_pipeline(feature_set):\n",
        "    print(\"Running prediction task for feature set\", \"+\".join(feature_set))\n",
        "    print(\"Generating labels...\")\n",
        "    labeled_pairs_df = get_labeled_pairs(pairs_df)\n",
        "    print(\"Computing paired features...\")\n",
        "    X_c0, X_c1, feature_names = get_feature_subset(labeled_pairs_df, feature_set)\n",
        "    X = X_c1 - X_c0\n",
        "    print(\"Using\", X.shape[1], \"features\")\n",
        "    y = labeled_pairs_df.first_convo_toxic.values\n",
        "    print(\"Running leave-one-page-out prediction...\")\n",
        "    accuracy, coefs, scores, hyperparams, pvalue = run_pred(X, y, feature_names, labeled_pairs_df.page_id)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"p-value: %.4e\" % pvalue)\n",
        "    print(\"C (mode):\", mode(hyperparams.logreg__C))\n",
        "    print(\"Percent of features (mode):\", mode(hyperparams.featselect__percentile))\n",
        "    print(\"Coefficents:\")\n",
        "    print(coefs.sort_values(by=\"mean_coef\"))\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTbnlm25Jp7P"
      },
      "source": [
        "## Step 8: Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8yJ0DQj1X1v"
      },
      "source": [
        "#feature_combos = [[\"emotion_analytics\"],[\"politeness_strategies\"], [\"prompt_types\"], [\"politeness_strategies\", \"prompt_types\"]]\n",
        "#feature_combos = [[\"politeness_strategies\", \"emotion_analytics\"], [\"prompt_types\", \"emotion_analytics\"]]\n",
        "feature_combos = [[\"politeness_strategies\"], [\"prompt_types\"], [\"emotion_analytics\"], [\"politeness_strategies\", \"prompt_types\"], [\"politeness_strategies\", \"emotion_analytics\"], [\"prompt_types\", \"emotion_analytics\"], [\"politeness_strategies\", \"prompt_types\", \"emotion_analytics\"]]\n",
        "combo_names = []\n",
        "accs = []\n",
        "for combo in feature_combos:\n",
        "    combo_names.append(\"+\".join(combo).replace(\"_\", \" \"))\n",
        "    accuracy = run_pipeline(combo)\n",
        "    accs.append(accuracy)\n",
        "results_df = pd.DataFrame({\"Accuracy\": accs}, index=combo_names)\n",
        "results_df.index.name = \"Feature set\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuUVn-2GnyVQ"
      },
      "source": [
        "results_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}